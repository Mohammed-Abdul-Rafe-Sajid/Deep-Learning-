{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNub+SmFVm+aDqqzAbJpkJX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammed-Abdul-Rafe-Sajid/Deep-Learning-/blob/main/LAB_4_Regularizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Experiment: Comparing Neural Network Regularization Techniques (MNIST)\n",
        "\n",
        "## üìå Objective\n",
        "\n",
        "To compare the performance of a neural network classification model using different regularization techniques and evaluate their effect on:\n",
        "\n",
        "- Model Accuracy  \n",
        "- Training Loss  \n",
        "- Validation Loss  \n",
        "- Generalization Ability  \n",
        "- Overfitting Reduction  \n",
        "\n",
        "---\n",
        "\n",
        "## üß† Dataset: MNIST\n",
        "\n",
        "### üîπ About MNIST\n",
        "\n",
        "- 70,000 grayscale images  \n",
        "- Image size: 28 √ó 28  \n",
        "- 10 classes (digits 0‚Äì9)  \n",
        "\n",
        "---\n",
        "\n",
        "## üßæ Data Preprocessing\n",
        "\n",
        "- Flatten images (28√ó28 ‚Üí 784)  \n",
        "- Normalize pixel values (0‚Äì255 ‚Üí 0‚Äì1)  \n",
        "- One-hot encode labels  \n",
        "\n",
        "---\n",
        "\n",
        "## üîí Regularization Techniques Used\n",
        "\n",
        "### 1Ô∏è‚É£ L1 Regularization\n",
        "- Adds penalty: |weights|  \n",
        "- Produces sparse weights  \n",
        "- Can eliminate less important features  \n",
        "\n",
        "### 2Ô∏è‚É£ L2 Regularization\n",
        "- Adds penalty: weights¬≤  \n",
        "- Prevents large weights  \n",
        "- Most commonly used  \n",
        "\n",
        "### 3Ô∏è‚É£ ElasticNet (L1 + L2)\n",
        "- Combines L1 and L2  \n",
        "- Balanced regularization  \n",
        "\n",
        "### 4Ô∏è‚É£ Dropout\n",
        "- Randomly drops neurons during training  \n",
        "- Reduces overfitting  \n",
        "\n",
        "### 5Ô∏è‚É£ Early Stopping\n",
        "- Stops training when validation loss stops improving  \n",
        "- Prevents unnecessary epochs  \n",
        "\n",
        "---\n",
        "\n",
        "## üìä Evaluation Metrics\n",
        "\n",
        "- Training Loss  \n",
        "- Validation Loss  \n",
        "- Test Accuracy  \n",
        "\n",
        "---\n",
        "\n",
        "## üìà Expected Observations\n",
        "\n",
        "| Technique | Overfitting | Accuracy | Generalization |\n",
        "|------------|------------|----------|----------------|\n",
        "| No Regularization | High | High train / Low val | Poor |\n",
        "| L1 | Medium | Moderate | Better |\n",
        "| L2 | Low | High | Very Good |\n",
        "| ElasticNet | Very Low | High | Excellent |\n",
        "| Dropout + EarlyStopping | Very Low | High | Best |\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Experiment Procedure\n",
        "\n",
        "- Load MNIST dataset  \n",
        "- Preprocess data  \n",
        "- Train 4 models:\n",
        "  - L1 Regularization  \n",
        "  - L2 Regularization  \n",
        "  - ElasticNet Regularization  \n",
        "  - Dropout + Early Stopping  \n",
        "- Compare accuracy and loss  \n",
        "- Visualize results  "
      ],
      "metadata": {
        "id": "kpr648QF4GzH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhUQDnV214r_",
        "outputId": "858e0a67-4326-4941-8d1a-2a64b2e8eb0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# MNIST Regularization Comparison Experiment\n",
        "# ==========================================\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load MNIST\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess\n",
        "X_train = X_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "X_test = X_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Function to build model\n",
        "def build_model(reg_type=None):\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    if reg_type == \"l1\":\n",
        "        reg = regularizers.l1(0.01)\n",
        "    elif reg_type == \"l2\":\n",
        "        reg = regularizers.l2(0.01)\n",
        "    elif reg_type == \"elastic\":\n",
        "        reg = regularizers.l1_l2(l1=0.01, l2=0.01)\n",
        "    else:\n",
        "        reg = None\n",
        "\n",
        "    model.add(layers.Dense(128, activation='relu', input_shape=(784,), kernel_regularizer=reg))\n",
        "    model.add(layers.Dense(64, activation='relu', kernel_regularizer=reg))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# ==============================\n",
        "# L1 Regularization\n",
        "# ==============================\n",
        "model_l1 = build_model(\"l1\")\n",
        "history_l1 = model_l1.fit(X_train, y_train, validation_split=0.2,\n",
        "                          epochs=20, batch_size=32, verbose=0)\n",
        "results[\"L1\"] = model_l1.evaluate(X_test, y_test, verbose=0)[1]\n",
        "\n",
        "# ==============================\n",
        "# L2 Regularization\n",
        "# ==============================\n",
        "model_l2 = build_model(\"l2\")\n",
        "history_l2 = model_l2.fit(X_train, y_train, validation_split=0.2,\n",
        "                          epochs=20, batch_size=32, verbose=0)\n",
        "results[\"L2\"] = model_l2.evaluate(X_test, y_test, verbose=0)[1]\n",
        "\n",
        "# ==============================\n",
        "# ElasticNet Regularization\n",
        "# ==============================\n",
        "model_elastic = build_model(\"elastic\")\n",
        "history_elastic = model_elastic.fit(X_train, y_train, validation_split=0.2,\n",
        "                                    epochs=20, batch_size=32, verbose=0)\n",
        "results[\"ElasticNet\"] = model_elastic.evaluate(X_test, y_test, verbose=0)[1]\n",
        "\n",
        "# ==============================\n",
        "# Dropout + Early Stopping\n",
        "# ==============================\n",
        "model_dropout = keras.Sequential([\n",
        "    layers.Dense(256, activation='relu', input_shape=(784,)),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_dropout.compile(optimizer='adam',\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history_dropout = model_dropout.fit(X_train, y_train,\n",
        "                                    validation_split=0.2,\n",
        "                                    epochs=100,\n",
        "                                    batch_size=32,\n",
        "                                    callbacks=[early_stop],\n",
        "                                    verbose=0)\n",
        "\n",
        "results[\"Dropout+EarlyStop\"] = model_dropout.evaluate(X_test, y_test, verbose=0)[1]\n",
        "\n",
        "# ==============================\n",
        "# Print Results\n",
        "# ==============================\n",
        "print(\"\\nTest Accuracy Comparison:\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "# ==============================\n",
        "# Bar Plot Comparison\n",
        "# ==============================\n",
        "plt.figure()\n",
        "plt.bar(results.keys(), results.values())\n",
        "plt.title(\"Regularization Technique Comparison\")\n",
        "plt.ylabel(\"Test Accuracy\")\n",
        "plt.xticks(rotation=30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tlncZ30K4MCQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}